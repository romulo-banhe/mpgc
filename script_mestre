# ==============================================================================
# SCRIPT MESTRE
#  ANÁLISE QUANTITATIVA DE DADOS QUALITATIVOS
# ==============================================================================

install.packages(c("tidyverse", "readxl", "entropy", "tidytext", 
                   "factoextra", "syuzhet", "corrplot", "igraph", 
                   "ggraph", "tm", "scales", "writexl","FactoMineR","factoextra" ))

# --- BLOCO 1: CARREGAMENTO DE BIBLIOTECAS ---
library(tidyverse)
library(readxl)
library(entropy)
library(tidytext)
library(factoextra)
library(syuzhet)
library(corrplot)
library(igraph)
library(ggraph)
library(tm)
library(scales)
library(FactoMineR)
library(factoextra)
# ==============================================================================
# ETAPA 1: CARREGAR OS DADOS (IMPORTAÇÃO)
# Certifique-se de que os arquivos Excel estão na pasta de trabalho
# ==============================================================================

# 1. Carregar Textos Completos (Para Entropia e TF-IDF)
# Colunas esperadas: ID, Setor, Texto_Completo
dados_texto <- read_excel("/Users/romulobanhe/Downloads/analise_mestrado/textos_completos.xlsx")

# 2. Carregar Matriz de Frequência (Para Dominância, Cluster e Correlação)
# Coluna 1 = Códigos, Colunas seguintes = Entrevistas (E1, E2...)
dados_freq <- read_excel("/Users/romulobanhe/Downloads/analise_mestrado/matriz_frequencia_nova.xlsx")

# 3. Carregar Matriz de Rede (Para Co-ocorrência)
# Matriz quadrada Código x Código
dados_rede <- read_excel("/Users/romulobanhe/Downloads/analise_mestrado/matriz_rede_nova.xlsx")

print("Dados carregados com sucesso! Iniciando análises...")

# ==============================================================================
# ETAPA 2: ENTROPIA DE SHANNON (RIQUEZA DO DISCURSO)
# ==============================================================================

# Função auxiliar para calcular entropia
calc_entropia <- function(txt) {
  # Limpeza básica e cálculo
  t <- tolower(removePunctuation(removeNumbers(as.character(txt))))
  w <- unlist(strsplit(t, "\\s+"))
  freq <- table(w)
  return(entropy.empirical(freq, unit="log2"))
}

# Aplicar cálculo
dados_texto$Entropia <- sapply(dados_texto$Texto_Completo, calc_entropia)

# Gráfico 1: Entropia
g1 <- ggplot(dados_texto, aes(x=reorder(ID, Entropia), y=Entropia)) +
  geom_col(fill="#4e79a7") +
  coord_flip() +
  theme_minimal() +
  labs(title="Densidade Informacional (Entropia de Shannon)", 
       subtitle="Riqueza do vocabulário dos gestores",
       x="Entrevistado", y="Entropia (Bits)")
print(g1)

# ==============================================================================
# ETAPA 3: DOMINÂNCIA DAS DIMENSÕES TOE
# ==============================================================================

# Preparar dados (Formato Longo)
dados_long <- dados_freq %>%
  pivot_longer(cols = -1, names_to = "Entrevista", values_to = "Frequencia") %>%
  rename(Codigo = 1)

# Classificar Códigos (Assumindo prefixos T_, O_, E_)
# IMPORTANTE: Se seus códigos não tiverem prefixo, precisará ajustar aqui
dados_long <- dados_long %>%
  mutate(Dimensao = case_when(
    str_detect(Codigo, "^T_") ~ "Tecnológica",
    str_detect(Codigo, "^O_") ~ "Organizacional",
    str_detect(Codigo, "^E_") ~ "Ambiental",
    str_detect(Codigo, "^EM_") ~ "Transversais",
    TRUE ~ "Outros"
  ))

# Gráfico 2: Barras Empilhadas
g2 <- ggplot(dados_long, aes(x = Entrevista, y = Frequencia, fill = Dimensao)) +
  geom_col(position = "fill") +
  scale_y_continuous(labels = percent) +
  theme_minimal() +
  labs(title = "Dominância das Dimensões TOE", 
       y = "% do Discurso", x = "Entrevistado") +
  scale_fill_brewer(palette = "Set2")
print(g2)

# ==============================================================================
# ETAPA 4: TF-IDF (TERMOS DISTINTIVOS POR SETOR)
# ==============================================================================

# Stopwords em português
stopwords_pt <- data.frame(palavra = stopwords("pt"))

lixo_fala <- c("_", "0", "1", "2", "3", "4", "5", "6", "7", "8", "9", "a", "à", "acaba", "acha", 
               "acho", "acordo", "acredito", "adeus", "agora", "aham", "aí", "ainda", "além", "algo", "alguém", 
               "algum", "alguma", "algumas", "alguns", "ali", "ampla", "amplas", "amplo", "amplos", "ano", 
               "anos", "ante", "antes", "ao", "aonde", "aos", "apenas", "apoio", "após", "aquela", "aquelas", 
               "aquele", "aqueles", "aqui", "aquilo", "área", "as", "às", "assim", "até", "atrás", "através", 
               "baixo", "bastante", "beleza", "bem", "boa", "boas", "bom", "bons", "breve", "cá", "cada", 
               "cara", "catorze", "cedo", "cento", "certamente", "certeza", "certo", "chega", "chego", "cima", 
               "cinco", "coisa", "coisas", "colocar", "com", "começa", "comigo", "como", "concordo", 
               "consegue", "conselho", "conta", "contra", "contudo", "cousa", "creio", "custa", "da", "dá", 
               "daí", "dão", "daquela", "daquelas", "daquele", "daqueles", "dar", "das", "dava", "de", 
               "debaixo", "dela", "delas", "dele", "deles", "demais", "dentro", "depois", "depressa", "desde", 
               "dessa", "dessas", "desse", "desses", "desta", "destas", "deste", "destes", "deu", "deve", 
               "devem", "devendo", "dever", "deverá", "deverão", "deveria", "deveriam", "devia", "deviam", 
               "dez", "dezanove", "dezesseis", "dezessete", "dezoito", "dia", "diante", "disse", "disso", 
               "disto", "dito", "diz", "dizem", "dizer", "dizia", "do", "dois", "dos", "dous", "doze", "duas", 
               "dúvida", "e", "é", "ela", "elas", "ele", "eles", "em", "embora", "enfim", "enquanto", "então", 
               "entendeu", "entrar", "entre", "era", "eram", "éramos", "és", "essa", "essas", "esse", "esses", 
               "esta", "está", "estamos", "estão", "estar", "estas", "estás", "estava", "estavam", "estávamos", 
               "este", "esteja", "estejam", "estejamos", "estes", "esteve", "estive", "estivemos", "estiver", 
               "estivera", "estiveram", "estivéramos", "estiverem", "estivermos", "estivesse", "estivessem", 
               "estivéssemos", "estiveste", "estivestes", "estou", "etc", "eu", "exatamente", "exato", 
               "exemplo", "faça", "faço", "fala", "falando", "falar", "falei", "falo", "falou", "falta", 
               "favor", "faz", "fazeis", "fazem", "fazemos", "fazendo", "fazer", "fazes", "fazia", "feita", 
               "feitas", "feito", "feitos", "fez", "fica", "ficar", "fico", "ficou", "fim", "final", "fiquei", 
               "fiz", "foi", "fomos", "for", "fora", "foram", "fôramos", "forem", "forma", "formos", "fosse", 
               "fossem", "fôssemos", "foste", "fostes", "fui", "gente", "geral", "grande", "grandes", "grupo", 
               "há", "hã", "haja", "hajam", "hajamos", "hão", "havemos", "havia", "hei", "hoje", "hora", 
               "horas", "houve", "houvemos", "houver", "houvera", "houverá", "houveram", "houvéramos", 
               "houverão", "houverei", "houverem", "houveremos", "houveria", "houveriam", "houveríamos", 
               "houvermos", "houvesse", "houvessem", "houvéssemos", "indo", "ir", "isso", "isto", "já", "jns", 
               "la", "lá", "lado", "lhe", "lhes", "lo", "local", "logo", "longe", "lugar", "maior", "maioria", 
               "mais", "mal", "mas", "máximo", "me", "meio", "melhor", "menor", "mba", "menos", "mês", "meses", 
               "mesma", "mesmas", "mesmo", "mesmos", "meu", "meus", "mil", "mim", "minha", "minhas", "momento", 
               "monte", "muita", "muitas", "muito", "muitos", "na", "nada", "não", "naquela", "naquelas", 
               "naquele", "naqueles", "nas", "né", "nele", "nem", "nenhum", "nenhuma", "nessa", "nessas", 
               "nesse", "nesses", "nesta", "nestas", "neste", "nestes", "ninguém", "nisso", "nível", "no", 
               "noite", "nome", "nos", "nós", "nossa", "nossas", "nosso", "nossos", "nova", "novas", "nove", 
               "novo", "novos", "num", "numa", "número", "nunca", "o", "obra", "obrigada", "obrigado", 
               "oitava", "oitavo", "oito", "olha", "onde", "ontem", "onze", "os", "ou", "outra", "outras", 
               "outro", "outros", "para", "parece", "parte", "partir", "paucas", "pega", "pela", "pelas", 
               "pelo", "pelos", "pequena", "pequenas", "pequeno", "pequenos", "per", "perante", "perfeito", 
               "perto", "pô", "pode", "pôde", "podem", "podendo", "poder", "poderia", "poderiam", "podia", 
               "podiam", "põe", "põem", "pois", "ponto", "pontos", "por", "porém", "porque", "porquê", 
               "posição", "possível", "possivelmente", "posso", "pouca", "poucas", "pouco", "poucos", "pra", 
               "primeira", "primeiras", "primeiro", "primeiros", "principalmente", "pro", "própria", 
               "próprias", "próprio", "próprios", "próxima", "próximas", "próximo", "próximos", "pude", 
               "puderam", "quais", "quáis", "qual", "quando", "quanto", "quantos", "quarta", "quarto", 
               "quatro", "que", "quê", "quem", "quer", "quereis", "querem", "queremas", "queres", "queria", 
               "quero", "questão", "quinta", "quinto", "quinze", "quis", "recentemente", "relação", "sabe", 
               "sabem", "sai", "saí", "são", "se", "segunda", "segundo", "sei", "seis", "seja", "sejam", 
               "sejamos", "sem", "sempre", "sendo", "senhor", "senhora", "ser", "será", "serão", "serei", 
               "seremos", "seria", "seriam", "seríamos", "sete", "sétima", "sétimo", "seu", "seus", "sexta", 
               "sexto", "si", "sido", "sim", "sistema", "só", "sob", "sobre", "sois", "somos", "sou", "sua", 
               "suas", "tá", "tal", "talvez", "também", "tampouco", "tanta", "tantas", "tanto", "tão", "tarde", 
               "te", "tem", "tém", "têm", "temos", "tendes", "tendo", "tenha", "tenham", "tenhamos", "tenho", 
               "tens", "ter", "terá", "terão", "terceira", "terceiro", "terei", "teremos", "teria", "teriam", 
               "teríamos", "teu", "teus", "teve", "ti", "tido", "tinha", "tinham", "tínhamos", "tive", 
               "tivemos", "tiver", "tivera", "tiveram", "tivéramos", "tiverem", "tivermos", "tivesse", 
               "tivessem", "tivéssemos", "tiveste", "tivestes", "tô", "toda", "todas", "todavia", "todo", 
               "todos", "trabalho", "tranquilo", "trás", "traz", "três", "treze", "tu", "tua", "tuas", "tudo", 
               "última", "últimas", "último", "últimos", "um", "uma", "umas", "uns", "usando", "usar", "vai", 
               "vais", "vamos", "vão", "vários", "vê", "veio", "vejo", "vem", "vêm", "vendo", "vens", "ver", 
               "verdade", "vez", "vezes", "vi", "viagem", "vim", "vindo", "vinte", "vir", "viu", "você", 
               "vocês", "vos", "vós", "vossa", "vossas", "vosso", "vossos", "vou", "zero", "pegar", "olhando", "vale", "comecei",
               "pegar", "falaria", "forte", "uso", "rampup", "mundo", "principais", "existe", "xxx", "xxxx","doutorado", "pedindo",
               "los", "las", "lo", "la", "me", "te", "se", "nos", "vos", "lhe","né", "pra", "pro", "tá", "tô", "aí", "então", "assim", "tipo", "sabe", "cara",
               "gente", "coisa", "parte", "lado", "exemplo", "agora", "hoje", "ano", "dia", "muito", "pouco", "bastante", "bem", "mal", "bom", "ruim", "tudo", "nada", "aeronáutico") 

stopwords_extras <- data.frame(palavra = lixo_fala)

# 3. Unir as duas listas
todas_stopwords <- bind_rows(stopwords_pt, stopwords_extras)

# 4. Processamento, Limpeza e Cálculo
palavras_setor <- dados_texto %>%
  unnest_tokens(palavra, Texto_Completo) %>%
  
  # AQUI ESTÁ O SEGREDO DA LIMPEZA:
  mutate(palavra = str_replace_all(palavra, "[^a-zçãõáéíóúâêôàü]", "")) %>% # Remove números e sujeira
  filter(nchar(palavra) > 2) %>%       # Remove palavras de 1 ou 2 letras (oi, eu, lá, cá)
  anti_join(todas_stopwords, by = "palavra") %>% # Remove a lista negra
  
  # Segue o cálculo normal
  count(Setor, palavra, sort = TRUE) %>%
  bind_tf_idf(palavra, Setor, n)

# 5. Gráfico 3: TF-IDF (Com limite rígido de 5 palavras)
g3 <- palavras_setor %>%
  group_by(Setor) %>%
  # AQUI ESTA A CORREÇÃO: with_ties = FALSE
  slice_max(tf_idf, n = 5, with_ties = FALSE) %>% 
  ungroup() %>%
  ggplot(aes(tf_idf, reorder_within(palavra, tf_idf, Setor), fill = Setor)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~Setor, scales = "free") +
  scale_y_reordered() +
  labs(title = "Gráfico 3: Assinatura Léxica por Setor (TF-IDF)", 
       subtitle = "Top 5 termos mais distintivos por vertical",
       x = "Importância Estatística (Score TF-IDF)", y = NULL) +
  theme_minimal()

print(g3)
# ==============================================================================
# ETAPA 5: DENDROGRAMA (CLUSTERIZAÇÃO HIERÁRQUICA)
# ==============================================================================

# Preparar Matriz
df_cluster <- as.data.frame(dados_freq)
rownames(df_cluster) <- df_cluster[[1]] # Nomes na linha
df_cluster <- df_cluster[,-1] # Remove coluna texto

# Transpor (Agrupar Pessoas) e Normalizar
df_cluster_t <- t(df_cluster) 
df_scaled <- scale(df_cluster_t)

# Calcular Distância
dist_mat <- dist(df_scaled, method = "euclidean")
hc <- hclust(dist_mat, method = "ward.D2")

# Gráfico 4: Dendrograma
# Nota: O dendrograma plotará direto na janela, não salva em variável 'g4'
fviz_dend(hc, k=3, rect=TRUE, cex=0.8, 
          k_colors = c("#2E9FDF", "#E7B800", "#FC4E07"),
          main="Gráfico 4: Clusterização dos Entrevistados (Similaridade)")

# ==============================================================================
# ETAPA 6: MATRIZ DE CORRELAÇÃO DE PEARSON (COM FILTRO DE SIGNIFICÂNCIA)
# ==============================================================================

# 1. Preparar Dados
# Transpor para ter Códigos como Colunas
df_corr <- t(df_cluster)

# 2. Selecionar TOP Códigos (Para o gráfico não ficar gigante)
# Aumentei para 20 para você ver mais conexões
variancias <- apply(df_corr, 2, var)
top_codigos <- names(sort(variancias, decreasing=TRUE))[1:20]
df_corr_top <- df_corr[, top_codigos]

# 3. Calcular a Correlação (r) e a Significância (p-value)
M <- cor(df_corr_top)
testRes <- cor.mtest(df_corr_top, conf.level = 0.95) # Teste de significância (95% confiança)

# 4. Gráfico 5: Correlograma Aprimorado
# Legenda:
# - Azul: Correlação Positiva (Crescem juntos)
# - Vermelho: Correlação Negativa (Um sobe, o outro desce)
# - X ou Branco: Não tem significância estatística (p > 0.05)

corrplot(M, 
         p.mat = testRes$p,       # Adiciona os p-values
         method = "circle",       # Bolinhas são mais fáceis de ler que cores chapadas
         type = "upper",          # Mostra só o triângulo de cima
         diag = FALSE,            # REMOVE A DIAGONAL DE 1.0 (Aqui resolve seu problema)
         sig.level = 0.05,        # Nível de corte (padrão acadêmico)
         insig = "blank",         # Deixa em BRANCO o que não é relevante (limpa o gráfico)
         tl.col = "black",        # Cor do texto
         tl.cex = 0.7,            # Tamanho do texto
         title = "Correlações Estatisticamente Significativas",
         mar = c(0,0,2,0),
         addCoef.col = "black",   # Mostra o número da correlação
         number.cex = 0.6         # Tamanho do número
)



# ==============================================================================
# ETAPA 7: REDE DE CO-OCORRÊNCIA
# ==============================================================================

# Preparar Dados
rownames(dados_rede) <- dados_rede[[1]]
matriz_rede <- as.matrix(dados_rede[,-1])

# Criar Grafo
g_network <- graph_from_adjacency_matrix(matriz_rede, mode="undirected", weighted=TRUE, diag=FALSE)

# Filtrar conexões fracas (peso < 1 ou 2, ajuste conforme sua necessidade)
g_filtered <- delete.edges(g_network, E(g_network)[weight < 1]) 
g_filtered <- delete.vertices(g_filtered, degree(g_filtered)==0)

# Gráfico 6: Rede
g6 <- ggraph(g_filtered, layout = "fr") +
  geom_edge_link(aes(width = weight), alpha=0.2, color="gray50") +
  geom_node_point(color="#69b3a2", size=5) +
  geom_node_text(aes(label = name), repel=TRUE, size=3) +
  theme_void() +
  labs(title="Rede de Co-ocorrência dos Códigos")
print(g6)

# ==============================================================================
# ETAPA 8: ANÁLISE DE EMOÇÕES (NRC LEXICON - CORRIGIDO)
# ==============================================================================

print("Iniciando análise de sentimentos NRC...")

# 1. Calcular Emoções
# O pacote 'syuzhet' já deve estar carregado lá no início
# Se der erro de "could not find function", rode: library(syuzhet)
sentimentos_nrc <- get_nrc_sentiment(dados_texto$Texto_Completo, language = "portuguese")

# 2. Preparar Dados para o Gráfico
# Calculamos a média de cada emoção para toda a amostra
# Colunas 1 a 8 são as emoções (raiva, antecipação, etc). 9 e 10 são Pos/Neg.
media_emocoes <- colMeans(sentimentos_nrc[, 1:8]) 
df_radar <- data.frame(Emoção = names(media_emocoes), Valor = media_emocoes)

# Traduzir nomes para o Gráfico ficar bonito em Português (Opcional)
# O NRC vem em inglês ou português dependendo da versão, vamos garantir:
traducao <- c(
  "anger" = "Raiva", 
  "anticipation" = "Antecipação", 
  "disgust" = "Aversão", 
  "fear" = "Medo", 
  "joy" = "Alegria", 
  "sadness" = "Tristeza", 
  "surprise" = "Surpresa", 
  "trust" = "Confiança"
)

# Tenta traduzir se o nome bater, senão mantém o original
df_radar$Emoção_Label <- ifelse(df_radar$Emoção %in% names(traducao), 
                                traducao[df_radar$Emoção], 
                                df_radar$Emoção)

# 3. Gráfico 7: Diagrama de Emoções (Coxcomb / Polar)
g7 <- ggplot(df_radar, aes(x = reorder(Emoção_Label, Valor), y = Valor, fill = Emoção_Label)) +
  geom_col(width = 0.95, color = "white") + # Cria as barras
  coord_polar() + # Transforma as barras em círculo (efeito radar)
  theme_minimal() +
  labs(title = "Gráfico 7: Perfil Emocional da Adoção de IA",
       subtitle = "Prevalência das emoções no discurso (NRC Lexicon)",
       y = "Intensidade", x = NULL) +
  scale_fill_brewer(palette = "Set3") + # Cores bonitas
  theme(
    axis.text.x = element_text(size = 11, face = "bold"), # Texto das emoções maior
    axis.text.y = element_blank(), # Remove os números do meio (poluição visual)
    legend.position = "bottom",
    legend.title = element_blank()
  )

print(g7)

# 4. (Opcional) Ver os números exatos no Console
print("Médias das Emoções:")
print(sort(media_emocoes, decreasing = TRUE))

# ==============================================================================
# ETAPA 9: ANÁLISE DE CORRESPONDÊNCIA (MAPA PERCEPTUAL - ANACOR)
# ==============================================================================

# 1. Preparar a Tabela (Entrevistados nas Linhas x Dimensões nas Colunas)
# Vamos usar os dados já limpos da Etapa 3 (dados_long)
tabela_ca <- dados_long %>%
  # Se quiser analisar CÓDIGOS específicos, mude 'Dimensao' para 'Codigo' na linha abaixo
  # Mas recomendo começar com Dimensao para ter um mapa limpo
  count(Entrevista, Dimensao, wt = Frequencia) %>% 
  pivot_wider(names_from = Dimensao, values_from = n, values_fill = 0) %>%
  column_to_rownames("Entrevista") # Transforma a coluna de nomes em índice

# 2. Rodar a ANACOR (CA)
res.ca <- CA(tabela_ca, graph = FALSE)

# 3. Gráfico 8: Mapa Perceptual (Biplot)
# Este gráfico coloca Pessoas (Azul) e Temas (Vermelho) no mesmo mapa
g10 <- fviz_ca_biplot(res.ca, 
                      repel = TRUE, # Evita texto sobreposto
                      title = "Mapa de Afinidade (Análise de Correspondência)",
                      col.row = "#0073C2FF",  # Cor dos Entrevistados (Azul)
                      col.col = "#E7B800"     # Cor das Dimensões/Temas (Amarelo)
) +
  theme_minimal() +
  labs(subtitle = "Proximidade entre Gestores e Dimensões (Distância Qui-Quadrado)")

print(g10)



# 4. (Opcional - Avançado) Ver a Contribuição dos Resíduos
# Mostra quais cruzamentos são estatisticamente mais fortes que o acaso
# Se o valor for > 2 (ou < -2), a atração/repulsão é muito forte.
print("Matriz de Resíduos Padronizados (O que define a atração):")
# Extrair a tabela de contingência esperada vs observada
chisq <- chisq.test(tabela_ca)
print(round(chisq$stdres, 2))


# ==============================================================================
# ETAPA 10: VALIDAÇÃO DA AMOSTRA - CURVA DE SATURAÇÃO (ORDEM CRONOLÓGICA REAL)
# ==============================================================================

# 1. Preparar os dados básicos
# Transpor: Linhas = Entrevistas, Colunas = Códigos
df_sat <- as.data.frame(t(dados_freq[,-1]))
colnames(df_sat) <- dados_freq[[1]]

# ------------------------------------------------------------------------------
# AQUI ESTÁ O AJUSTE: DEFINIR A ORDEM REAL DOS ACONTECIMENTOS
# ------------------------------------------------------------------------------
ordem_real <- c("E1", "E2", "E3", "E4", "E6", "E7", "E8", 
                "E10", "E11", "E9", "E12", "E13", "E14", "E5")

# Reordenar o dataframe para seguir a linha do tempo, não a ordem alfabética
# O R vai pegar a linha do E9 e colocar depois da linha do E11
df_sat_ordenado <- df_sat[match(ordem_real, rownames(df_sat)), ]

# Ajustar o fator para o gráfico manter essa ordem no eixo X
df_sat_ordenado$Entrevista <- factor(rownames(df_sat_ordenado), levels = ordem_real)

# 2. Função de Cálculo (Mesma de antes)
calcular_saturacao <- function(df) {
  # Identifica colunas de códigos (todas menos a coluna 'Entrevista' se existir)
  # Como adicionamos 'Entrevista' no final ali em cima, removemos ela do cálculo numérico
  dados_num <- df[, sapply(df, is.numeric)]
  
  novos_por_entrevista <- numeric(nrow(df))
  acumulado <- numeric(nrow(df))
  
  codigos_ja_vistos <- rep(FALSE, ncol(dados_num))
  
  for(i in 1:nrow(df)) {
    linha <- as.numeric(dados_num[i, ])
    codigos_presentes <- linha > 0
    
    # É novo se está presente AGORA e NUNCA foi visto nas linhas de CIMA
    novos <- codigos_presentes & !codigos_ja_vistos
    
    novos_por_entrevista[i] <- sum(novos)
    codigos_ja_vistos <- codigos_ja_vistos | codigos_presentes
    acumulado[i] <- sum(codigos_ja_vistos)
  }
  
  return(data.frame(
    Entrevista = df$Entrevista,
    Novos = novos_por_entrevista,
    Acumulado = acumulado
  ))
}

# Calcular usando a tabela ORDENADA
dados_saturacao <- calcular_saturacao(df_sat_ordenado)

# 3. Gráfico 9: Curva de Saturação (Cronologia Real)
g11 <- ggplot(dados_saturacao, aes(x = as.numeric(Entrevista))) +
  geom_line(aes(y = Acumulado, color = "Total Acumulado"), size = 1.2) +
  geom_point(aes(y = Acumulado, color = "Total Acumulado"), size = 3) +
  geom_col(aes(y = Novos, fill = "Novos Temas (Marginal)"), alpha = 0.6) +
  
  # Aqui garante que o eixo X mostre os nomes na ordem certa
  scale_x_continuous(breaks = 1:14, labels = dados_saturacao$Entrevista) +
  
  theme_minimal() +
  labs(title = "Curva de Saturação Teórica (Ordem Cronológica)",
       subtitle = "Evolução da descoberta de temas conforme a sequência real das entrevistas",
       x = "Sequência das Entrevistas", y = "Quantidade de Códigos",
       color = "", fill = "") +
  scale_color_manual(values = c("Total Acumulado" = "#2E9FDF")) +
  scale_fill_manual(values = c("Novos Temas (Marginal)" = "#FC4E07")) +
  theme(legend.position = "bottom")

print(g11)
# ==============================================================================
# FIM DA ANÁLISE
# ==============================================================================

